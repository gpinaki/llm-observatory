# ğŸ§  LLM Insights Hub
A high-performance, enterprise-focused platform designed to support CTOs and enterprise architects in making informed decisions about large language model (LLM) usage and efficiency. Leveraging Streamlit, the LLM Insights Hub allows for real-time monitoring, cost analysis, and performance benchmarking of leading LLM providers such as OpenAI and Anthropic.

## ğŸŒŸ Features
Multi-Provider LLM Support: Easily switch between OpenAI and Anthropic LLMs within a unified interface.
Detailed Cost and Performance Metrics: Provides granular tracking of tokens, cost, and response times, allowing teams to analyze LLM efficiency across models.
Configurable Model Settings: Customizable parameters, including temperature and token limits, enable precise tuning of model outputs.
Real-Time Chat Console: Interact directly with your chosen LLM provider to see responses and metrics in real time.
Session Summary and Advanced Insights: Review an overview of cumulative session metrics, including total cost, token usage, and average response time.

## ğŸ“Š Application Overview
LLM Insights Hub is purpose-built for enterprise applications, helping organizations assess model cost-effectiveness and response quality. Its primary features include:

Control Panel: Configure application settings like deployment environment, provider, and model parameters for flexibility across use cases.
Performance Overview: Track and visualize key performance indicators, including cost and token usage per session.
Session Summary: Display session-wide statistics in a compact sidebar format to help users quickly assess cumulative costs and response efficiency.
Advanced Configuration: Fine-tune model settings, such as temperature and token limits, for application-specific requirements.

## ğŸ“š How It Works

###LLM Insights Hub enables organizations to evaluate LLM performance by examining cost and efficiency metrics. Key workflows include:

####Provider Selection: Choose between OpenAI and Anthropic models, each with distinct cost and performance attributes.
###Model Customization: Adjust response characteristics through settings like temperature and token limits.
###Cost and Usage Tracking: Monitor usage metrics such as token count, total cost, and average response time per session to optimize resource allocation.
## ğŸ”§ Configuration Options

## Environment: Select from Development, Test, Integration, or Production.
## LLM Provider: Choose between OpenAI and Anthropic for flexible model use.
## Advanced Settings: Customize model response behavior by modifying temperature and maximum tokens.

## ğŸ› ï¸ Project Structure


llm-observatory/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # Configuration settings for API keys and environments
â”‚   â”œâ”€â”€ interface/
â”‚   â”‚   â””â”€â”€ app.py              # Main application file for Streamlit
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ openai_llm.py       # OpenAI LLM integration module
â”‚   â”‚   â””â”€â”€ anthropic_llm.py    # Anthropic LLM integration module
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ README.md                   # Project documentation

## ğŸ¤ Contributing
Contributions are welcome! Please fork the repository, create a feature branch, and submit a pull request. For any issues or suggestions, feel free to raise an issue in the GitHub repository.

